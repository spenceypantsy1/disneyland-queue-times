{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5200dce",
   "metadata": {},
   "source": [
    "# Combining raw dataframes information together to form a master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7da018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "using waiting_times as the base for master as it contains \n",
    "all relevant information for attraction specific wait times\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "master = pd.read_parquet(\"../data/raw_data/waiting_times.parquet\")\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# |  link attraction park merging   |\n",
    "# ===================================\n",
    "\n",
    "# link attraction park to label which attraction belongoning to which park\n",
    "link_attraction_park = pd.read_parquet(\"../data/processed_data/link_attraction_park.parquet\")\n",
    "master_merged_1 = master.merge(link_attraction_park, left_on = 'ENTITY_DESCRIPTION_SHORT', right_on = 'ATTRACTION', how = 'left')\n",
    "\n",
    "# replacing ENTITY_DESCRIPTION_SHORT with ATTRACTION for clarity\n",
    "master_merged_1.drop(columns = ['ENTITY_DESCRIPTION_SHORT'], inplace = True)  \n",
    "\n",
    "\n",
    "# ===================================\n",
    "# |           attendance            |\n",
    "# ===================================\n",
    "\n",
    "# link attendance to mark attendance for each park and date. this way wait times will have daily attendance\n",
    "attendance = pd.read_parquet(\"../data/raw_data/attendance.parquet\")\n",
    "\n",
    "# converting datetimes to datetime format\n",
    "attendance['USAGE_DATE'] = pd.to_datetime(attendance['USAGE_DATE'])\n",
    "master_merged_1['WORK_DATE'] = pd.to_datetime(master_merged_1['WORK_DATE'])\n",
    "\n",
    "master_merged_2 = master_merged_1.merge(attendance, left_on = ['PARK', 'WORK_DATE'], right_on = ['FACILITY_NAME', 'USAGE_DATE'], how = 'left')\n",
    "#! there are a bunch of dates that dont have attendance data, keep for now\n",
    "\n",
    "# removing FACILITY_NAME and USAGE_DATE since redundant after merge\n",
    "master_merged_2.drop(columns = ['FACILITY_NAME', 'USAGE_DATE'], inplace = True)\n",
    "master_merged_2\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# |         weather data            |\n",
    "# ===================================\n",
    "\n",
    "# link weather data to mark weather for each park and date. this way wait times will have daily weather\n",
    "weather_data = pd.read_parquet(\"../data/raw_data/weather_data.parquet\")\n",
    "\n",
    "# weather data time is in iso format and UTC, converting it to datetime with timezone offset\n",
    "weather_data['dt_iso'] = pd.to_datetime(weather_data['dt'], unit = 's') + pd.to_timedelta(weather_data['timezone'], unit='s')\n",
    "\n",
    "master_merged_3 = master_merged_2.merge(weather_data, left_on = 'WORK_DATE', right_on = 'dt_iso', how = 'left')\n",
    "\n",
    "# removing all redundant datetime/ timeszone/ location columns\n",
    "master_merged_3.drop(columns = ['dt', 'dt_iso', 'timezone', 'city_name', 'lat', 'lon'], inplace = True)\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# |        entity schedule          |\n",
    "# ===================================\n",
    "\n",
    "# link entity schedule to indicate when there are closures for whichever attraction\n",
    "entity_schedule = pd.read_parquet(\"../data/raw_data/entity_schedule.parquet\")\n",
    "\n",
    "# creating a subset so i dont merge the entire entity schedule with redundant columns\n",
    "entity_schedule_subset = entity_schedule[['REF_CLOSING_DESCRIPTION', 'ENTITY_DESCRIPTION_SHORT', 'WORK_DATE']].copy()\n",
    "\n",
    "# converting datetimes to datetime format\n",
    "entity_schedule_subset['WORK_DATE'] = pd.to_datetime(entity_schedule_subset['WORK_DATE'])\n",
    "\n",
    "master_merged_4 = master_merged_3.merge(entity_schedule_subset, left_on = ['ATTRACTION', 'WORK_DATE'], right_on = ['ENTITY_DESCRIPTION_SHORT', 'WORK_DATE'], how = 'left')\n",
    "\n",
    "# dropping redundant merge columns\n",
    "master_merged_4.drop(columns = [ 'ENTITY_DESCRIPTION_SHORT'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_merged_4.to_parquet('../data/processed_data/master.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a6bee",
   "metadata": {},
   "source": [
    "# Adding Months feature from WORK_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "436258e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning WORK_DATE into datetime then extracting months as a feature\n",
    "master_merged_4['WORK_DATE'] = pd.to_datetime(master_merged_4['WORK_DATE'])\n",
    "master_merged_4['month'] = master_merged_4['WORK_DATE'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db1ecd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORK_DATE</th>\n",
       "      <th>DEB_TIME_x</th>\n",
       "      <th>DEB_TIME_HOUR</th>\n",
       "      <th>FIN_TIME_x</th>\n",
       "      <th>WAIT_TIME_MAX</th>\n",
       "      <th>NB_UNITS</th>\n",
       "      <th>GUEST_CARRIED</th>\n",
       "      <th>CAPACITY</th>\n",
       "      <th>ADJUST_CAPACITY</th>\n",
       "      <th>OPEN_TIME</th>\n",
       "      <th>...</th>\n",
       "      <th>weather_id</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "      <th>REF_CLOSING_DESCRIPTION</th>\n",
       "      <th>ENTITY_DESCRIPTION_SHORT</th>\n",
       "      <th>ENTITY_TYPE</th>\n",
       "      <th>DEB_TIME_y</th>\n",
       "      <th>FIN_TIME_y</th>\n",
       "      <th>UPDATE_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 21:00:00.000</td>\n",
       "      <td>21</td>\n",
       "      <td>2018-01-01 21:15:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>802</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roller Coaster</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2018-01-01 10:00:00.000</td>\n",
       "      <td>2018-01-01 17:44:00.000</td>\n",
       "      <td>2018-01-02 07:46:01.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 19:30:00.000</td>\n",
       "      <td>19</td>\n",
       "      <td>2018-01-01 19:45:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>254.749</td>\n",
       "      <td>254.75</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>802</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bumper Cars</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2018-01-01 08:30:00.000</td>\n",
       "      <td>2018-01-01 21:05:00.000</td>\n",
       "      <td>2018-01-02 07:51:31.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 22:30:00.000</td>\n",
       "      <td>22</td>\n",
       "      <td>2018-01-01 22:45:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>802</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rapids Ride</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2018-01-01 09:29:00.000</td>\n",
       "      <td>2018-01-01 17:08:00.000</td>\n",
       "      <td>2018-01-02 07:51:31.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 12:45:00.000</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-01-01 13:00:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>250.001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>802</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crazy Dance</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2018-01-01 09:12:00.000</td>\n",
       "      <td>2018-01-01 22:00:00.000</td>\n",
       "      <td>2018-01-02 07:43:47.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 17:00:00.000</td>\n",
       "      <td>17</td>\n",
       "      <td>2018-01-01 17:15:00.000</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>211.500</td>\n",
       "      <td>198.25</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>802</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Skyway</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2018-01-01 10:00:00.000</td>\n",
       "      <td>2018-01-01 19:00:00.000</td>\n",
       "      <td>2018-01-02 07:55:16.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509319</th>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>2022-08-18 18:45:00.000</td>\n",
       "      <td>18</td>\n",
       "      <td>2022-08-18 19:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "      <td>Fermeture Réhab</td>\n",
       "      <td>Himalaya Ride</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-19 07:56:33.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509320</th>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>2022-08-18 10:15:00.000</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-08-18 10:30:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "      <td>Fermeture Réhab</td>\n",
       "      <td>Crazy Dance</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-19 07:56:29.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509321</th>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>2022-08-18 09:15:00.000</td>\n",
       "      <td>9</td>\n",
       "      <td>2022-08-18 09:30:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "      <td>Fermeture Réhab</td>\n",
       "      <td>Crazy Dance</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-19 07:56:29.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509322</th>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>2022-08-18 20:30:00.000</td>\n",
       "      <td>20</td>\n",
       "      <td>2022-08-18 20:45:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "      <td>Fermeture Réhab</td>\n",
       "      <td>Giga Coaster</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-18 23:59:00.000</td>\n",
       "      <td>2022-08-19 07:56:35.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509323</th>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>2022-08-18 10:45:00.000</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-08-18 11:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oz Theatre</td>\n",
       "      <td>ATTR</td>\n",
       "      <td>2022-08-18 12:00:00.000</td>\n",
       "      <td>2022-08-18 17:24:00.000</td>\n",
       "      <td>2022-08-19 07:56:31.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3509324 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         WORK_DATE               DEB_TIME_x  DEB_TIME_HOUR  \\\n",
       "0       2018-01-01  2018-01-01 21:00:00.000             21   \n",
       "1       2018-01-01  2018-01-01 19:30:00.000             19   \n",
       "2       2018-01-01  2018-01-01 22:30:00.000             22   \n",
       "3       2018-01-01  2018-01-01 12:45:00.000             12   \n",
       "4       2018-01-01  2018-01-01 17:00:00.000             17   \n",
       "...            ...                      ...            ...   \n",
       "3509319 2022-08-18  2022-08-18 18:45:00.000             18   \n",
       "3509320 2022-08-18  2022-08-18 10:15:00.000             10   \n",
       "3509321 2022-08-18  2022-08-18 09:15:00.000              9   \n",
       "3509322 2022-08-18  2022-08-18 20:30:00.000             20   \n",
       "3509323 2022-08-18  2022-08-18 10:45:00.000             10   \n",
       "\n",
       "                      FIN_TIME_x  WAIT_TIME_MAX  NB_UNITS  GUEST_CARRIED  \\\n",
       "0        2018-01-01 21:15:00.000              0       2.0            0.0   \n",
       "1        2018-01-01 19:45:00.000              5      18.0          148.0   \n",
       "2        2018-01-01 22:45:00.000              0       1.0            0.0   \n",
       "3        2018-01-01 13:00:00.000              5       1.0           46.0   \n",
       "4        2018-01-01 17:15:00.000              5      15.0           92.0   \n",
       "...                          ...            ...       ...            ...   \n",
       "3509319  2022-08-18 19:00:00.000              0       0.0            0.0   \n",
       "3509320  2022-08-18 10:30:00.000              0       0.0            0.0   \n",
       "3509321  2022-08-18 09:30:00.000              0       0.0            0.0   \n",
       "3509322  2022-08-18 20:45:00.000              0       0.0            0.0   \n",
       "3509323  2022-08-18 11:00:00.000              0       2.0            0.0   \n",
       "\n",
       "         CAPACITY  ADJUST_CAPACITY  OPEN_TIME  ...  weather_id  weather_main  \\\n",
       "0           0.000             0.00          0  ...         802        Clouds   \n",
       "1         254.749           254.75         15  ...         802        Clouds   \n",
       "2           0.000             0.00          0  ...         802        Clouds   \n",
       "3         250.001           250.00         15  ...         802        Clouds   \n",
       "4         211.500           198.25         15  ...         802        Clouds   \n",
       "...           ...              ...        ...  ...         ...           ...   \n",
       "3509319     0.000             0.00          0  ...         804        Clouds   \n",
       "3509320     0.000             0.00          0  ...         804        Clouds   \n",
       "3509321     0.000             0.00          0  ...         804        Clouds   \n",
       "3509322     0.000             0.00          0  ...         804        Clouds   \n",
       "3509323     0.000             0.00          0  ...         804        Clouds   \n",
       "\n",
       "         weather_description weather_icon REF_CLOSING_DESCRIPTION  \\\n",
       "0           scattered clouds          03n                     NaN   \n",
       "1           scattered clouds          03n                     NaN   \n",
       "2           scattered clouds          03n                     NaN   \n",
       "3           scattered clouds          03n                     NaN   \n",
       "4           scattered clouds          03n                     NaN   \n",
       "...                      ...          ...                     ...   \n",
       "3509319      overcast clouds          04n         Fermeture Réhab   \n",
       "3509320      overcast clouds          04n         Fermeture Réhab   \n",
       "3509321      overcast clouds          04n         Fermeture Réhab   \n",
       "3509322      overcast clouds          04n         Fermeture Réhab   \n",
       "3509323      overcast clouds          04n                     NaN   \n",
       "\n",
       "         ENTITY_DESCRIPTION_SHORT  ENTITY_TYPE               DEB_TIME_y  \\\n",
       "0                  Roller Coaster         ATTR  2018-01-01 10:00:00.000   \n",
       "1                     Bumper Cars         ATTR  2018-01-01 08:30:00.000   \n",
       "2                     Rapids Ride         ATTR  2018-01-01 09:29:00.000   \n",
       "3                     Crazy Dance         ATTR  2018-01-01 09:12:00.000   \n",
       "4                          Skyway         ATTR  2018-01-01 10:00:00.000   \n",
       "...                           ...          ...                      ...   \n",
       "3509319             Himalaya Ride         ATTR  2022-08-18 23:59:00.000   \n",
       "3509320               Crazy Dance         ATTR  2022-08-18 23:59:00.000   \n",
       "3509321               Crazy Dance         ATTR  2022-08-18 23:59:00.000   \n",
       "3509322              Giga Coaster         ATTR  2022-08-18 23:59:00.000   \n",
       "3509323                Oz Theatre         ATTR  2022-08-18 12:00:00.000   \n",
       "\n",
       "                      FIN_TIME_y              UPDATE_TIME  \n",
       "0        2018-01-01 17:44:00.000  2018-01-02 07:46:01.000  \n",
       "1        2018-01-01 21:05:00.000  2018-01-02 07:51:31.000  \n",
       "2        2018-01-01 17:08:00.000  2018-01-02 07:51:31.000  \n",
       "3        2018-01-01 22:00:00.000  2018-01-02 07:43:47.000  \n",
       "4        2018-01-01 19:00:00.000  2018-01-02 07:55:16.000  \n",
       "...                          ...                      ...  \n",
       "3509319  2022-08-18 23:59:00.000  2022-08-19 07:56:33.000  \n",
       "3509320  2022-08-18 23:59:00.000  2022-08-19 07:56:29.000  \n",
       "3509321  2022-08-18 23:59:00.000  2022-08-19 07:56:29.000  \n",
       "3509322  2022-08-18 23:59:00.000  2022-08-19 07:56:35.000  \n",
       "3509323  2022-08-18 17:24:00.000  2022-08-19 07:56:31.000  \n",
       "\n",
       "[3509324 rows x 44 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "filtering out extreme outliers for numerical features by\n",
    "removing any data points outside of 3*IQR for each numerical column\n",
    "\"\"\"\n",
    "df_clean= df.copy()\n",
    "\n",
    "#get the numerical columns\n",
    "numerical_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1  = df_clean[col].quantile(0.25)\n",
    "    Q3  = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 3*IQR\n",
    "    upper_bound = Q3 + 3*IQR\n",
    "\n",
    "    #count outliers before removal\n",
    "    outlier_count = len(df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)])\n",
    "    print(f\"Column: {col}, has {outlier_count} outliers\")\n",
    "\n",
    "    #remove the outliers\n",
    "    df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "display(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6537f",
   "metadata": {},
   "source": [
    "standard normal transformation\n",
    "\n",
    "$$ (x-µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c303b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance columns: ['PARK', 'weather_main', 'weather_description', 'weather_icon', 'REF_CLOSING_DESCRIPTION', 'month']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nfd56p353h9gnytlmplrwn880000gn/T/ipykernel_36671/1102225279.py:7: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n"
     ]
    }
   ],
   "source": [
    "#finding imbalance categories\n",
    "\n",
    "#converting dateetime strings into datetime dtype\n",
    "df_clean['DEB_TIME']= pd.to_datetime(df_clean['DEB_TIME'])\n",
    "df_clean['FIN_TIME']= pd.to_datetime(df_clean['FIN_TIME'])\n",
    "\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "imbalance_cols = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    value_counts = df_clean[col].value_counts()\n",
    "    percentages = value_counts / len(df_clean) * 100\n",
    "\n",
    "    #check highest occuring category and lowest occurring category to find imbalance ratio\n",
    "    imbalance_ration =  (percentages.max()-percentages.min())\n",
    "\n",
    "    #check if imbalance ratio is 5 > times\n",
    "    if imbalance_ration > 5:\n",
    "        imbalance_cols.append(col)\n",
    "print(f\"Imbalance columns: {imbalance_cols}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20753fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DEB_TIME', 'DEB_TIME_HOUR', 'FIN_TIME', 'WAIT_TIME_MAX', 'NB_UNITS',\n",
       "       'GUEST_CARRIED', 'CAPACITY', 'ADJUST_CAPACITY', 'OPEN_TIME', 'UP_TIME',\n",
       "       'DOWNTIME', 'NB_MAX_UNIT', 'ATTRACTION', 'PARK', 'attendance', 'temp',\n",
       "       'dew_point', 'feels_like', 'temp_min', 'temp_max', 'pressure',\n",
       "       'humidity', 'wind_speed', 'wind_deg', 'clouds_all', 'weather_id',\n",
       "       'weather_main', 'weather_description', 'weather_icon',\n",
       "       'REF_CLOSING_DESCRIPTION'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "082f15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (1893178, 32)\n",
      "Train Set Shape: (1135906, 32)\n",
      "Validation Set hape: (378636, 32)\n",
      "Test Set Shape: (378636, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split our data before scaling to avoid data leakage\n",
    "train_df, test_df = train_test_split(df_clean, \n",
    "                                     test_size=0.4, \n",
    "                                     random_state=42,\n",
    "                                     stratify=df_clean[['PARK','weather_main', 'weather_description','weather_icon','REF_CLOSING_DESCRIPTION']])\n",
    "\n",
    "val_df, test_df = train_test_split(test_df, \n",
    "                                     test_size=0.5, \n",
    "                                     random_state=42,\n",
    "                                     stratify=test_df[['PARK','weather_main', 'weather_description','weather_icon','REF_CLOSING_DESCRIPTION']]) \n",
    "\n",
    "print(f\"Original Shape: {df_clean.shape}\")\n",
    "print(f\"Train Set Shape: {train_df.shape}\")\n",
    "print(f\"Validation Set hape: {val_df.shape}\")\n",
    "print(f\"Test Set Shape: {test_df.shape}\")\n",
    "\n",
    "numerical_cols = df_clean.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e842467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1135906, 21)\n",
      "Validation set shape: (378636, 21)\n",
      "Test set shape: (378636, 21)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numerical_cols = train_df.select_dtypes(include=['number']).columns\n",
    "# even though its numeric it should be treated as categorical\n",
    "numerical_cols = numerical_cols.drop('DEB_TIME_HOUR')\n",
    "\n",
    "# scale fit on train set ONLY\n",
    "train_df_scaled = scaler.fit_transform(train_df[numerical_cols])\n",
    "\n",
    "# scale transform on val and test set using the same scaler fit on train set\n",
    "val_df_scaled = scaler.transform(val_df[numerical_cols])\n",
    "test_df_scaled = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "train_df_scaled = pd.DataFrame(train_df_scaled, columns = numerical_cols, index = train_df.index)\n",
    "val_df_scaled = pd.DataFrame(val_df_scaled, columns = numerical_cols, index = val_df.index)\n",
    "test_df_scaled = pd.DataFrame(test_df_scaled, columns = numerical_cols, index = test_df.index)\n",
    "\n",
    "print(f\"Train set shape: {train_df_scaled.shape}\")\n",
    "print(f\"Validation set shape: {val_df_scaled.shape}\")\n",
    "print(f\"Test set shape: {test_df_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba7267",
   "metadata": {},
   "source": [
    "# Categorical Enconding\n",
    "One hot encoding because we dont have any ordinal categories for ordinal or binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9778e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nfd56p353h9gnytlmplrwn880000gn/T/ipykernel_36671/3024805385.py:3: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1135906, 70)\n",
      "Validation set shape: (378636, 70)\n",
      "Test set shape: (378636, 70)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# adding hours to categorical\n",
    "categorical_cols.append('DEB_TIME_HOUR')\n",
    "\n",
    "# dropping redundant merge columns\n",
    "master_merged_4.drop(columns = ['WORK_DATE', 'ENTITY_DESCRIPTION_SHORT'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5c437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
